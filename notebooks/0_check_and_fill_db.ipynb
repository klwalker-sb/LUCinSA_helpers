{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d941c9",
   "metadata": {},
   "source": [
    "# Get cell processing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a56fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import pickle\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"../LUCinSA_helpers\")\n",
    "from file_checks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMETERS: modify in notebook_params notebook, then run that notebook and this cell to update here\n",
    "DO NOT modify this cell\n",
    "'''\n",
    "\n",
    "%store -r basic_config\n",
    "print(\"basic parameters: \\n brdf_dir = {} \\n grid_cell = {} \\n index_dir = {} \\n home_dir = {}\"\n",
    "      .format(basic_config['brdf_dir'],basic_config['grid_cell'],basic_config['index_dir'],basic_config['home_dir']))\n",
    "%store -r single_output_params\n",
    "print(\"single_output_params: \\n map_years = {}\".format(single_output_params['map_years']))\n",
    "%store -r single_plot_params\n",
    "print(\"single_plot_params: \\n image_type = {}\".format(single_plot_params['image_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_db(processing_info_path,landsat_path,sentinel2_path,brdf_path,modified):\n",
    "    '''\n",
    "    This checks for an existing processing database and creates one if needed from download and brdf folders.\n",
    "    All downloaded images are assumed to be in download folders (landsat and sentinel2 still). If cleaning has\n",
    "    already occured, this won't work. TODO: make option to construct from brdf folder only if cleaning has already occured.\n",
    "    Note: this will not create the numpix and coreg shift_x and shift_y columns in the original db, nor any error notes,\n",
    "    so best to use original database whenever possible.\n",
    "    '''\n",
    "    if os.path.exists(landsat_path):\n",
    "        num_landsat_files = len([fi for fi in os.listdir(landsat_path) if fi.endswith('.tif')])\n",
    "    else:\n",
    "        num_landsat_files = 0\n",
    "    if os.path.exists(sentinel2_path):\n",
    "        num_sentinel2_files = len([fi for fi in os.listdir(sentinel2_path) if fi.endswith('.tif')])\n",
    "    else:\n",
    "        num_sentinel2_files = 0\n",
    "\n",
    "    if num_landsat_files + num_sentinel2_files == 0:\n",
    "        print('no (new)images have been downloaded')\n",
    "    else:\n",
    "        ## Make new processing db if it does not already exist:\n",
    "        if not processing_info_path.is_file():\n",
    "            processing_dict = {}\n",
    "            if num_landsat_files > 0:\n",
    "                landsat_files = [fi for fi in os.listdir(landsat_path) if fi.endswith('.tif')]\n",
    "                for f in landsat_files:\n",
    "                    processing_dict[os.path.splitext(f)[0]]={'dl':'{}/{}'.format(landsat_path,f),'beforeDB':True}\n",
    "            if num_sentinel2_files > 0:\n",
    "                sentinel2_files = [fi for fi in os.listdir(sentinel2_path) if fi.endswith('.tif')]\n",
    "                for s in sentinel2_files:\n",
    "                    processing_dict[os.path.splitext(s)[0]]={'dl':'{}/{}'.format(sentinel2_path,s),'beforeDB':True}\n",
    "            new_processing_info = pd.DataFrame.from_dict(processing_dict,orient='index')\n",
    "            new_processing_info.rename_axis('id', axis=1, inplace=True)\n",
    "            pd.to_pickle(new_processing_info, processing_info_path)\n",
    "            print(f'{len(new_processing_info)} images downloaded and added to database.')\n",
    "\n",
    "    processing_db = pd.read_pickle(processing_info_path)\n",
    "    ## to fix issues from older version of db already created for some cells:\n",
    "    if 'id' not in processing_db:\n",
    "        processing_db.rename_axis('id', axis=1, inplace=True)\n",
    "    #if processing_db.index != 'id':\n",
    "    #    print('removing original index column and setting it to id column')\n",
    "    #    processing_db.set_index('id', drop=True, inplace=True)\n",
    "        \n",
    "    print(f'{len(processing_db)} records in db. {num_landsat_files} landsat and {num_sentinel2_files} sentinel images in downloads.')\n",
    "\n",
    "    if len(processing_db) >= num_landsat_files + num_sentinel2_files:\n",
    "        print('all downloaded images have probably been added to db already')\n",
    "    else:\n",
    "        print('adding images to db...')\n",
    "        new_dls = {}\n",
    "        landsat_files = [fi for fi in os.listdir(landsat_path) if fi.endswith('.tif')]\n",
    "        for f in landsat_files:\n",
    "            if os.path.splitext(f)[0] in processing_db.values:\n",
    "                continue\n",
    "            else:\n",
    "                new_dls[os.path.splitext(f)[0]]={'dl':'{}/{}'.format(landsat_path,f),'beforeDB':True}\n",
    "        sentinel2_files = [fi for fi in os.listdir(sentinel2_path) if fi.endswith('.tif')]\n",
    "        for s in sentinel2_files:\n",
    "            if os.path.splitext(s)[0] in processing_db.values:\n",
    "                continue\n",
    "            else:\n",
    "                new_dls[os.path.splitext(s)[0]]={'dl':'{}/{}'.format(sentinel2_path,s),',beforeDB':True}\n",
    "        \n",
    "        if len(new_dls)>0:\n",
    "            new_dl_db = pd.DataFrame.from_dict(new_dls,orient='index')\n",
    "            new_dl_db.rename_axis('id', axis=1, inplace=True)\n",
    "            processing_db.append(new_dl_db)\n",
    "            modified = True\n",
    "            \n",
    "    if os.path.exists(brdf_path):\n",
    "        if 'brdf' in processing_db:\n",
    "            print('brdf data already in database')\n",
    "            \n",
    "        else: \n",
    "            print('adding brdf info to db...')\n",
    "            processing_db['brdf_id'] = np.nan\n",
    "            processing_db['brdf_error'] = np.nan\n",
    "            processing_db['brdf'] = np.nan\n",
    "            processing_db['bandpass'] = np.nan\n",
    "            for idx, row in processing_db.iterrows():\n",
    "                match=None\n",
    "                #print(idx)\n",
    "                for fi in os.listdir(brdf_path):\n",
    "                    if fi.endswith('.nc'):\n",
    "                        if idx.startswith('S'):  \n",
    "                            if (idx.split('_')[1] in fi.split('_')[2]) and (idx.split('_')[2] == fi.split('_')[3]):\n",
    "                                match = fi\n",
    "                        elif idx.startswith('L'): \n",
    "                            if (idx.split('_')[0] == fi.split('_')[1]) and (idx.split('_')[2] in fi.split('_')[2]) and (idx.split('_')[3] == fi.split('_')[3]):\n",
    "                                match = fi\n",
    "                #print(f'match:{match}')\n",
    "                processing_db.at[idx,'brdf_id']=match\n",
    "                if match is not None:\n",
    "                    if match.split('_')[0] == 'L3B':\n",
    "                        processing_db.at[idx,'bandpass']=True\n",
    "                    elif match.split('_')[0] == 'L3A':\n",
    "                        processing_db.at[idx,'bandpass']=False\n",
    "                \n",
    "            modified = True\n",
    "            \n",
    "        num_coreged_files = len([fi for fi in os.listdir(brdf_path) if fi.endswith('coreg.nc')])\n",
    "        print(f'{num_coreged_files} images have been coreged')\n",
    "        if num_coreged_files == 0:\n",
    "            print('coregistration has not yet occured. Processing database is up to date')\n",
    "        else:\n",
    "            if 'shift_x' in processing_db:\n",
    "                print('coreg data has already been added to database')\n",
    "            else:\n",
    "                print('adding coreg info to db...')\n",
    "                processing_db['coreg'] = np.nan\n",
    "                processing_db['shift_x'] = np.nan\n",
    "                processing_db['shift_y'] = np.nan\n",
    "                processing_db['coreg_error'] = np.nan\n",
    "                for idx, row in processing_db.iterrows():\n",
    "                    match=None\n",
    "                    #print(idx)\n",
    "                    for fi in os.listdir(brdf_path):\n",
    "                        if fi.endswith('.nc'):\n",
    "                            if idx.startswith('S'):\n",
    "                                if (idx.split('_')[1] in fi.split('_')[2]) and (idx.split('_')[2] == fi.split('_')[3]):\n",
    "                                    match = fi\n",
    "                            ## right now we are only coreging Sentinel, so this makes no sense \n",
    "                            elif idx.startswith('L'): \n",
    "                                if (idx.split('_')[0] == fi.split('_')[1]) and (idx.split('_')[2] in fi.split('_')[2]) and (idx.split('_')[3] == fi.split('_')[3]):\n",
    "                                    match = fi\n",
    "                    #print(f'match:{match}')\n",
    "                    if match is not None:\n",
    "                        if 'coreg' in match:\n",
    "                            processing_db.at[idx,'coreg']=True\n",
    "                        else:\n",
    "                            processing_db.at[idx,'coreg']=False\n",
    "                            if idx.startswith('S'):\n",
    "                                processing_db.at[idx,'coreg_error']='unknown'\n",
    "                \n",
    "                modified = True\n",
    "                        \n",
    "    else:\n",
    "        print('brdfs have not yet been created. Processing database is up to date')\n",
    "\n",
    "    if modified == True:\n",
    "        pd.to_pickle(processing_db, processing_info_path)\n",
    "        print('saving new database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for cellid in range(3871,3872):\n",
    "    processing_info_path = Path('{}/{:06d}/processing.info'.format(basicConfig['raw_dir'],cellid))\n",
    "    landsat_path = Path('{}/{:06d}/landsat'.format(basicConfig['raw_dir'],cellid))\n",
    "    sentinel2_path = Path('{}/{:06d}/sentinel2'.format(basicConfig['raw_dir'],cellid))\n",
    "    brdf_path = Path('{}/{:06d}/brdf'.format(basicConfig['raw_dir'],cellid))\n",
    "\n",
    "    print('processing {}...'.format(cellid))\n",
    "    if not os.path.exists(landsat_path):\n",
    "        continue\n",
    "    if processing_info_path.is_file():\n",
    "        reconstructed_dbs = []\n",
    "        deleted_dbs = []\n",
    "        processing_db = pd.read_pickle(processing_info_path)\n",
    "        if 'shift_x' in processing_db:\n",
    "            print ('already has db with shift x')\n",
    "            if len(processing_db['brdf_id'].unique()) < 10:\n",
    "                print('this db was created without unique brdf ids')\n",
    "                processing_db.drop(['brdf','bandpass','brdf_error','brdf_id','coreg','shift_x','shift_y','coreg_error'], axis=1, inplace=True)\n",
    "                pd.to_pickle(processing_db, processing_info_path)\n",
    "                reconstructed_dbs.append(cellid)\n",
    "        elif 'numpix' in processing_db and 'bdrf_id' in processing_db:\n",
    "            if len(processing_db['brdf_id'].unique()) < 10:\n",
    "                print('this db was created without unique brdf ids')\n",
    "                processing_db.drop(['brdf','bandpass','brdf_error','brdf_id'], axis=1, inplace=True)\n",
    "                pd.to_pickle(processing_db, processing_info_path)\n",
    "                reconstructed_dbs.append(cellid)\n",
    "        elif 'numpix' not in processing_db:\n",
    "            print('deleting existing db')\n",
    "            processing_info_path.unlink()\n",
    "            deleted_dbs.append(cellid)\n",
    "    else:\n",
    "        print('no existing database. making new database')\n",
    "        \n",
    "    reconstruct_db(processing_info_path,landsat_path,sentinel2_path,brdf_path,modified=False)\n",
    "print('restructured dbs:{}'.format(reconstructed_dbs))\n",
    "print('deleted dbs:{}'.format(deleted_dbs))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_info_path = Path('{}/{:06d}/processing.info'.format(basic_config['raw_dir'],int(basic_config['grid_cell'])))\n",
    "landsat_path = Path('{}/{:06d}/landsat'.format(basic_config['raw_dir'],int(basic_config['grid_cell'])))\n",
    "sentinel2_path = Path('{}/{:06d}/sentinel2'.format(basic_config['raw_dir'],int(basic_config['grid_cell'])))\n",
    "brdf_path = Path(basic_config['brdf_dir'])\n",
    "modified = False\n",
    "reconstruct_db(processing_info_path,landsat_path,sentinel2_path,brdf_path,modified=False)\n",
    "processing_db = pd.read_pickle(Path('{}/{:06d}/processing.info'.format(basic_config['raw_dir'],int(basic_config['grid_cell']))))\n",
    "#processing_db.set_index('id', drop=True, inplace=True)\n",
    "#processing_db.drop(['brdf','bandpass','brdf_id','coreg','brdf_error','shift_x','shift_y','coreg_error'], axis=1, inplace=True)\n",
    "#pd.to_pickle(processing_db, processing_info_path)\n",
    "processing_db.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "brdf_db = pd.read_pickle(Path(brdf_path/'scene.info'))\n",
    "brdf_db.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa34197",
   "metadata": {},
   "outputs": [],
   "source": [
    "##View processing errors\n",
    "processing_errors1 = processing_db[processing_db['redownload']==True]\n",
    "processing_errors2 = processing_db[~processing_db['brdf_error'].isnull()]\n",
    "processing_errors = pd.concat([processing_errors1, processing_errors2],axis=0)\n",
    "print('of the {} images available, {} were not processed due to errors'.format(processing_db.shape[0],processing_errors.shape[0]))\n",
    "processing_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##View brdf status\n",
    "processed0 = processing_db[processing_db['skip']!=True]\n",
    "processed = processed0[processed0['redownload']!=True]\n",
    "no_brdf = processed[processed['brdf']==False | processed['brdf'].isnull()]\n",
    "print('of the {} images processed, {} do not have brdf calculations'.format(processed.shape[0],no_brdf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##View coreg status:\n",
    "processed_sentinel = processed[processed.index.str.startswith('S')]\n",
    "creg_sentinel = processed_sentinel[processed_sentinel['coreg']==True]\n",
    "print('of the {} Sentinel images, {} were coreged'.format(processed_sentinel.shape[0],creg_sentinel.shape[0]))\n",
    "avg_x_shift = creg_sentinel['shift_x'].mean()\n",
    "avg_y_shift = creg_sentinel['shift_y'].mean()\n",
    "med_x_shift = creg_sentinel['shift_x'].median()\n",
    "med_y_shift = creg_sentinel['shift_y'].median()\n",
    "print ('shift x: avg:{}, med:{}. shift y: avg:{}, med:{}'.format(avg_x_shift, avg_y_shift, med_x_shift, med_y_shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###To get all images in brdf directory:\n",
    "all_images = print_files_in_directory(basic_config['brdf_dir'],'.nc',print_list=basic_config['print_list'],out_dir=basic_config['home_dir'],data_source='stac')\n",
    "\n",
    "if basic_config['print_list'] == True:\n",
    "    print('full dataframe is printed as FileList.txt in {}'.format(out_dir=basic_config['home_dir']))\n",
    "else:\n",
    "    print('sample of dataframe: (Not printed to file. Can print by setting printList=True in notebook_params)')\n",
    "all_images.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ceced8",
   "metadata": {},
   "source": [
    "## Read scene.info file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac118a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "p_df = pd.read_pickle(Path('{}/{:06d}/processing.info'.format(basic_config['raw_dir'],int(basic_config['grid_cell']))))\n",
    "p_df = p_df.reset_index()\n",
    "p_df['sensor'] = p_df.apply(lambda x: x['index'].split('_')[0], axis=1)\n",
    "p_df['shift'] = p_df.apply(lambda x: math.sqrt(math.pow(x['shift_x'],2)+math.pow(x['shift_y'],2)),axis=1)\n",
    "p_df.set_index('index',inplace=True, drop=True)\n",
    "#p_df5 = p_df[p_df['sensor']=='LT05']\n",
    "p_df7 = p_df[p_df['sensor']=='LE07']\n",
    "p_df7.head(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6686ce",
   "metadata": {},
   "source": [
    "# Get cell status from new db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5265583",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for all years:\n",
    "df_all = get_img_list_from_db(basic_config['raw_dir'], basic_config['grid_cell'],single_plot_params['image_type'],yrs=None,data_source='stac')\n",
    "##for selection of years:\n",
    "df_slice = get_img_list_from_db(basic_config['raw_dir'], basic_config['grid_cell'],single_plot_params['image_type'],yrs=single_output_params['map_years'],data_source='stac')\n",
    "\n",
    "df_slice.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = get_cell_status(basic_config['raw_dir'], '/home/downspout-cel/paraguay_lc/stac/grids', basic_config['grid_cell'],yrs=None,data_source='stac')\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133acdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_status_db_path = '/home/downspout-cel/paraguay_lc/cell_processing_dl.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_db_path = '/home/downspout-cel/paraguay_lc/cell_processing_post.csv'\n",
    "#update_cell_status_db(status_db_path, range(4050,4101), basic_config['raw_dir'], '/home/downspout-cel/paraguay_lc/stac/grids', yrs=None,data_source='stac')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb38cee",
   "metadata": {},
   "source": [
    "## To save an html copy of this notebook with all outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b26464",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run to print output as html\n",
    "outName = str(basic_config['country']+'0_check_and_fill_db_'+str(basic_config['grid_cell']))\n",
    "!jupyter nbconvert --output-dir='./Outputs' --to html --no-input --output=$outName 0_check_and_fill_db.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b2a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
