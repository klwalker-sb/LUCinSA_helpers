{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a947286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d48bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"../LUCinSA_helpers\")\n",
    "from rf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f479b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMETERS: modify in notebook_params notebook, then run that notebook and this cell to update here\n",
    "DO NOT modify this cell\n",
    "'''\n",
    "\n",
    "%store -r basic_config\n",
    "print(\"Basic Parameters: \\n time-series data is in (smooth_dir): {} \\n\"\n",
    "      \" modelling year is (filter_year param): {} (this is first year if season spans two years)\"\n",
    "      .format(basic_config['smooth_dir'], basic_config['filter_yr']))\n",
    "%store -r classification_params\n",
    "\n",
    "print(\"Classification_Params: \\n\" \n",
    "      \" modelling mode is {} \\n\"\n",
    "      \" model_type = {} \\n\"\n",
    "      \" output files are saved to (model_dir): {} \\n\" \n",
    "      \" shared input files are in (main_model_dir): {} \\n\"\n",
    "      \" sample_model = {} \\n feature_model = {} \\n model_name = {} \\n\"\n",
    "      \" the full sample pt file: {} \\n\"\n",
    "      \" the full sample dataframe with the feature model applied: {} \\n\"\n",
    "      \" the subset pt file based on the sample model: {} \\n\"\n",
    "      \" % of the sample heldout for the confusion matrices: {} \\n\"\n",
    "      \" lc_class = {} \\n ranhold = {} \\n impmeth = {}\"\n",
    "      .format(classification_params['model_mode'],classification_params['model_type'],classification_params['model_dir'],\n",
    "              classification_params['main_model_dir'],classification_params['sample_model'],classification_params['feature_model'],\n",
    "              classification_params['model_name'],basic_config['ptfile'],classification_params['samp_pix_vars'],\n",
    "              classification_params['samp_pts'],classification_params['ho_thresh'],\n",
    "              classification_params['lc_mod'],classification_params['ranhold'],classification_params['impmeth']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_variables(feature_model, sample_model, year):\n",
    "    ## Simpler way to change model configuration than updating notebook parameters\n",
    "    samp_pt_key = basic_config['ptfile']\n",
    "    lut='../Class_LUT.csv'\n",
    "    classification_params['feature_model'] = feature_model\n",
    "    classification_params['sample_model'] = sample_model\n",
    "    classification_params['model_name'] = f'{feature_model}_{sample_model}'\n",
    "    model_name = classification_params['model_name']\n",
    "    classification_params['samp_pix_vars'] = f'/home/downspout-cel/paraguay_lc/vector/pts_training/features/ptsfeats_{feature_model}_{year}.csv'\n",
    "    classification_params[\"samp_pts\"] = f'/home/downspout-cel/paraguay_lc/vector/pts_training/pt_subsets/{sample_model}_{year}.csv'\n",
    "    print(f\"Now working with sample_model:{classification_params['sample_model']} \\n New output model will be named:{classification_params['model_name']}\")\n",
    "    print('inputs are coming from: \\n    samp_df:{} \\n feature_df: {} \\n'.format(classification_params[\"samp_pts\"],classification_params['samp_pix_vars']))\n",
    "\n",
    "    return classification_params\n",
    "\n",
    "def apply_smalls(pixdf,lut,outpath=None):\n",
    "    if 'LC2' not in list(pixdf.columns):\n",
    "        pixdf = pixdf.merge(lut[['LC_UNQ','LC2']],on='LC_UNQ',how='left')\n",
    "    ### <=1 hectare\n",
    "    if 'var_poly_area' in pixdf.columns.values.tolist():\n",
    "        pixdf['smlhld_1ha'] = pixdf.apply(lambda x: 1 if (\n",
    "            ((x['var_poly_area'] < 100) and (x['LC2'] == 30) and (x['LC25'] < 40)) or (\n",
    "            (x['Width'] <= 100) and (x['LC2'] == 30)) or x['LC25'] in ['Crops-mix','Crops-Mandioca','Crops-Horticulture','Crops-Sesame']) else 0, axis=1)\n",
    "    else:\n",
    "        pixdf['smlhld_1ha'] = pixdf.apply(lambda x: 1 if ((\n",
    "            (x['Width'] <= 100) and (x['LC2'] == 30)) or x['LC25'] in ['Crops-mix','Crops-Mandioca','Crops-Horticulture','Crops-Sesame']) else 0, axis=1)\n",
    "    num_smlhld_1ha = pixdf['smlhld_1ha'].sum()\n",
    "    print(f'{num_smlhld_1ha} of the sample points are small fields < 100 m across')\n",
    "    ### <= .5 hectare\n",
    "    if 'var_poly_area' in pixdf.columns.values.tolist():\n",
    "        pixdf['smlhd_halfha'] = pixdf.apply(lambda x: 1 if (\n",
    "        ((x['var_poly_area'] < 50) and (x['LC2'] == 30) and (x['LC25'] < 40)) or (\n",
    "        (x['Width'] <= 50) and (x['LC2'] == 30)) or x['LC25'] in ['Crops-mix','Crops-Mandioca','Crops-Horticulture','Crops-Sesame']) else 0, axis=1)\n",
    "    else:\n",
    "        pixdf['smlhd_halfha'] = pixdf.apply(lambda x: 1 if ((\n",
    "        (x['Width'] <= 50) and (x['LC2'] == 30)) or x['LC25'] in ['Crops-mix','Crops-Mandioca','Crops-Horticulture','Crops-Sesame']) else 0, axis=1)\n",
    "    num_smlhld_halfha = pixdf['smlhd_halfha'].sum()\n",
    "    print(f'{num_smlhld_halfha} of the sample points are very small fields < 50 m across')\n",
    "    if outpath:\n",
    "        pd.DataFrame.to_csv(pixdf, outpath)\n",
    "    \n",
    "    return pixdf\n",
    "\n",
    "def append_vars_to_samp(lut,pt_path, var_path,out_path):\n",
    "\n",
    "    lut=pd.read_csv(lut)\n",
    "    #print(lut.sort_values('LC_UNQ')[['LC_UNQ','USE_NAME','LC25','LC25_name']])\n",
    "    pt_key = pd.read_csv(basic_config['ptfile'])\n",
    "\n",
    "    samp_pts = pd.read_csv(pt_path)\n",
    "    print(f'There are {samp_pts.shape[0]} sample points')\n",
    "    if ('OID_' not in list(samp_pts.columns)) & ('Unnamed: 0' in list(samp_pts.columns)):\n",
    "        samp_pts.rename(columns = {\"Unnamed: 0\": 'OID_'}, inplace = True)\n",
    "    #print(samp_pts.columns.tolist())\n",
    "    if 'LC_UNQ_y' in list(samp_pts.columns):\n",
    "        samp_pts.drop(['LC_UNQ_y'], axis=1, inplace=True)\n",
    "    ## dropping LC2 from sample points bc used different classification system in previous versions \n",
    "    if 'LC2' in list(samp_pts.columns):\n",
    "        samp_pts.drop(['LC2'], axis=1, inplace=True)\n",
    "    if 'LC25' not in list(samp_pts.columns):\n",
    "        if 'LC_UNQ' in list(samp_pts.columns):\n",
    "            samp_pts = samp_pts.merge(lut[['LC_UNQ','LC25','LC25_name']],on='LC_UNQ',how='left')\n",
    "        else:\n",
    "            samp_pts.merge(lut[['LC_UNQ','LC25','LC25_name']],left_on='LC', right_on='LC_UNQ',how='left')\n",
    "    pix_vars = pd.read_csv(var_path)\n",
    "    if 'var_poly_area' in list(pix_vars.columns):\n",
    "        ## hacky fix for issue of numbers over signed 16-bit max being converted to negative in var dataframe \n",
    "        pix_vars['var_poly_area'] = np.where(pix_vars['var_poly_area']<0,32767,pix_vars['var_poly_area'])\n",
    "    pix_vars = pd.merge(pix_vars,pt_key[['OID_','PID']],on='OID_', how='left')\n",
    "    pix_data = samp_pts.merge(pix_vars, left_on='PID', right_on='PID', how='inner')\n",
    "    if 'LC_UNQ_x' in list(pix_data.columns):\n",
    "        pix_data.rename(columns={'LC_UNQ_x': 'LC_UNQ'},inplace=True)\n",
    "    if ('USE_NAME' in list(pix_data.columns)) and ('Class' not in list(pix_data.columns)):\n",
    "         pix_data.rename(columns={'USE_NAME':'Class'},inplace=True)\n",
    "    if 'smlhld_1ha' not in list(pix_data.columns):\n",
    "        pix_data = apply_smalls(pix_data,lut)\n",
    "    if 'OID__x' in list(pix_data.columns):\n",
    "        pix_data.rename(columns={'OID__x': 'OID_'},inplace=True)\n",
    "        pix_data.drop(['OID__y'], axis=1, inplace=True)\n",
    "    \n",
    "    #print('sample breakdown by LC25 class:')\n",
    "    #print(pix_data['LC25_name'].value_counts())\n",
    "\n",
    "    ## Note rand2 <= .2 has already been pulled off these datasets\n",
    "    pix_data['TESTSET10'] = np.where(pix_data['rand2'] > .91, 1, 0)\n",
    "    pix_data['TESTSET20'] = np.where(pix_data['rand2'] > .82, 1, 0)\n",
    "    pd.DataFrame.to_csv(pix_data, out_path)\n",
    "\n",
    "def create_temp_rf_model(lut,fixed_ho_dir,mod_dir,class_mod,runnum):\n",
    "    classification_params['ho_thresh'] = 0\n",
    "    class_mod_name = get_class_col(class_mod,lut)\n",
    "    model_name = classification_params['model_name'] + '_' + class_mod_name[0]   \n",
    "    df_in = classification_params[\"pixdf\"]\n",
    "    print(f'making model for {model_name}_{runnum}')\n",
    "    #rfpath = os.path.join(mod_dir,f'{model_name}_RFmod{runnum}.joblib')\n",
    "    \n",
    "    # Note model needs to be created from command line to save properly for use in classification. models created here are for testing only\n",
    "    if mod_dir == classification_params['main_model_dir']:\n",
    "        if os.path.isfile(rfpath):\n",
    "            print('STOP -- DO NOT OVERWRITE SAVED MODEL. Change model_mode or model_dir to proceed')\n",
    "            sys.exit() \n",
    "\n",
    "    rf0 = rf_model(df_in,\n",
    "        mod_dir,\n",
    "        class_mod,\n",
    "        classification_params['impmeth'],\n",
    "        classification_params['ranhold'],\n",
    "        classification_params['model_name'],\n",
    "        lut,\n",
    "        classification_params['feature_model'],\n",
    "        classification_params['ho_thresh'],         \n",
    "        classification_params['feature_mod_dict'],\n",
    "        update_model_dict=False,\n",
    "        fixed_ho=True,\n",
    "        fixed_ho_dir=fixed_ho_dir,\n",
    "        runnum=runnum)\n",
    "        \n",
    "    return rf0\n",
    "\n",
    "def log_acc_results(scores_dict, model_name, these_scores,runnum):\n",
    "    try:\n",
    "        with open(scores_dict, 'r+') as full_dict:\n",
    "            dic = json.load(full_dict)\n",
    "        if runnum:\n",
    "            model_name = f'{model_name}_{runnum}'\n",
    "        else:\n",
    "            model_name = model_name \n",
    "        dic.update({model_name : these_scores})\n",
    "          \n",
    "    except IOError:\n",
    "        print('File not found, will create a new one.')\n",
    "        dic = {model_name : rf0[1]}\n",
    "\n",
    "    ## Add counter for multiple runs\n",
    "    \n",
    "    new_scores = pd.DataFrame.from_dict(dic)\n",
    "    print(new_scores.head())\n",
    "    #new_scores.to_csv(model_scores_tab)\n",
    "\n",
    "    with open(scores_dict, 'w') as new_dict:\n",
    "        json.dump(dic, new_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a35c1-51a9-4593-847e-0193d0a6a29c",
   "metadata": {},
   "source": [
    "## Get fixed holdout (if not already done)\n",
    "All sample models use the same HO set; just need to create if changing the feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ac43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the holdout was already taken out prior to creating the sample point files\n",
    "### if GENERAL_HO files exist, this part should not need to be rerun\n",
    "#fixed_ho_dir = '/home/downspout-cel/paraguay_lc/vector/pts_calval/fixed_HOs'\n",
    "#ho, tr0 = get_stable_holdout(pix_data, fixed_ho_dir, 20, 'smallCrop', lut, overwrite=False) \n",
    "#ho1, tr1 = get_stable_holdout(tr0, fixed_ho_dir, 20, 'bigCrop', lut, overwrite=False) \n",
    "#ho2, tr2 = get_stable_holdout(tr1, fixed_ho_dir, 20, 'noCrop', lut, overwrite=False)\n",
    "#pixdf = pd.read_csv('/home/downspout-cel/paraguay_lc/vector/pts_training/GENERAL_TRAINING.csv')\n",
    "\n",
    "### run this part if using a new feature model\n",
    "'''\n",
    "var_path = classification_params['samp_pix_vars']\n",
    "for ho in ['noCrop','smallCrop','medCrop', 'bigCrop']:\n",
    "    pt_path = f'/home/downspout-cel/paraguay_lc/vector/pts_calval/fixed_HOs/GENERAL_HOLDOUT_{ho}.csv'\n",
    "    out_path = f'/home/downspout-cel/paraguay_lc/vector/pts_calval/fixed_HOs/{feature_model}_HOLDOUT_{ho}.csv'\n",
    "    append_vars_to_samp(lut, pt_path, var_path, out_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d081503-83b0-41d5-9202-2cb1be3cfb28",
   "metadata": {},
   "source": [
    "### Set new variables here for temp model testing: -- SKIP if keeping original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a783532",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE SURE THIS IS USING POST HO INFO\n",
    "year = 2021\n",
    "\n",
    "#feature_model = \"base4Poly6\"\n",
    "feature_model = \"base4NoPoly\"\n",
    "#feature_model = \"base4NoPolyLonly\"\n",
    "#feature_model = \"base4NoPoly30m\"\n",
    "\n",
    "sample_model = 'bal300mix5'\n",
    "lut='../Class_LUT.csv'\n",
    "\n",
    "#fixed_ho_dir = '/home/downspout-cel/paraguay_lc/vector/pts_calval/EPy_district_samp'\n",
    "fixed_ho_dir = '/home/downspout-cel/paraguay_lc/vector/pts_calval/fixed_HOs'\n",
    "class_mod = 'all'\n",
    "\n",
    "model_scores_dict = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_scores_dict_F.json'\n",
    "model_scores_tab = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_scores_F.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1092532",
   "metadata": {},
   "source": [
    "## For singular experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set_variables(feature_model, sample_model, year)\n",
    "model_name = classification_params['model_name']\n",
    "print(model_name)\n",
    "out_path = f'/home/downspout-cel/paraguay_lc/classification/inputs/pixdf_{model_name}_{year}.csv'\n",
    "classification_params[\"pixdf\"] = out_path\n",
    "append_vars_to_samp(lut,pt_path, var_path, out_path)\n",
    "\n",
    "rf = create_temp_rf_model(lut,fixed_ho_dir,mod_dir,class_mod,runnum=0)\n",
    "#print(rf[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04722ad7",
   "metadata": {},
   "source": [
    "## To make multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdabb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_multiple_models(score_dict, temp_dir, lut, fixed_ho_dir, numruns=10):\n",
    "    for m in [0]:\n",
    "        for b in range(0,11):\n",
    "            sample_model = f'bal{m}mix{b}'\n",
    "            classification_params = set_variables(feature_model, sample_model, 2021)\n",
    "            model_name = classification_params['model_name']\n",
    "            print(f'building {model_name}...')\n",
    "            out_path = f'/home/downspout-cel/paraguay_lc/classification/inputs/pixdf_{model_name}_{year}.csv'\n",
    "            classification_params[\"pixdf\"] = out_path\n",
    "            var_path = classification_params['samp_pix_vars']\n",
    "            pt_path = classification_params[\"samp_pts\"]\n",
    "            if not os.path.exists(out_path):\n",
    "                append_vars_to_samp(lut,pt_path, var_path, out_path)\n",
    "            for rn in range(6,(numruns + 1)):\n",
    "                rf0 = create_temp_rf_model(lut,fixed_ho_dir,temp_dir,class_mod,rn)\n",
    "                log_acc_results(score_dict, model_name, rf0[1],rn)\n",
    "\n",
    "model_score_dict = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_iterations_LC25_2021.json'\n",
    "temp_mod_dir = '/home/scratch-cel/rf_mods'\n",
    "iterate_multiple_models(model_score_dict, temp_mod_dir, lut, fixed_ho_dir, numruns=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_run_scores(score_dict, agg_score_tab):\n",
    "    with open(score_dict, 'r+') as full_dict:\n",
    "        dic = json.load(full_dict)\n",
    "    new_scores = pd.DataFrame.from_dict(dic).T\n",
    "    agg_scores = new_scores.groupby(['F','S','C','A']).agg(avgF1=('F1_cnc', 'mean'),stdF1=('F1_cnc', 'std'),avgOA=('OA_cnc','mean'),stdOA=('OA_cnc','std'),\n",
    "                                                        recallsc=('recall_smallCrop','mean'),stdsc=('recall_smallCrop','std'),\n",
    "                                                        recallbc=('recall_bigCrop','mean'),stdbc=('recall_bigCrop','std'),\n",
    "                                                        recallnc=('recall_noCrop','mean'),stdnc=('recall_noCrop','std'))\n",
    "    agg_scores.to_csv(agg_score_tab)\n",
    "\n",
    "    return agg_scores\n",
    "    \n",
    "def get_best_models(score_dict, final_models_tab):\n",
    "    with open(score_dict, 'r+') as full_dict:\n",
    "        dic = json.load(full_dict)\n",
    "    new_scores = pd.DataFrame.from_dict(dic).T\n",
    "    new_scores['F1_cnc'] = new_scores['F1_cnc'].astype('float64')\n",
    "    best_models = new_scores.groupby(['F','S','C','A'])['F1_cnc'].idxmax()\n",
    "    keep_models = new_scores.loc[best_models]\n",
    "    keep_models.to_csv(final_models_tab)\n",
    "\n",
    "    return keep_models\n",
    "\n",
    "model_score_dict = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_iterations_LC25_2021.json'\n",
    "agg_score_tab = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_iteration_LC25_summary.csv'\n",
    "aggregate_run_scores(model_score_dict,agg_score_tab)\n",
    "final_models_tab = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_best_models_LC25.csv'\n",
    "keep_models = get_best_models(model_score_dict, final_models_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f741fdf-8225-46e0-838e-ad8a22cebe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "class MyScalarFormatter(ScalarFormatter):\n",
    "    # Override '_set_format' with your own\n",
    "    def _set_format(self):\n",
    "        self.format = '%.2f'  # Show 2 decimals\n",
    "        \n",
    "agg_score_tab = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_iteration_LC25_summary.csv'\n",
    "agg_scores = pd.read_csv(agg_score_tab)\n",
    "#print(agg_scores)\n",
    "agg_scores[['bal', 'mix']] = agg_scores['S'].str.split('mix', n=1, expand=True)\n",
    "agg_scores['mix']=agg_scores['mix'].astype('int')\n",
    "feature_mod = 'base4Poly6'\n",
    "bal0 = agg_scores[(agg_scores['bal'] == 'bal0') & (agg_scores['F'] == feature_mod)]\n",
    "bal100 = agg_scores[(agg_scores['bal'] == 'bal100') & (agg_scores['F'] == feature_mod)]\n",
    "bal200 = agg_scores[(agg_scores['bal'] == 'bal200') & (agg_scores['F'] == feature_mod)]\n",
    "bal300 = agg_scores[(agg_scores['bal'] == 'bal300') & (agg_scores['F'] == feature_mod)]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "custom_formatter = MyScalarFormatter(useMathText=True)\n",
    "\n",
    "axs[0, 0].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[0, 0].errorbar(bal0['mix'],bal0['recallnc'], yerr = bal0['stdnc'], fmt ='o', label ='min class size = 0')\n",
    "axs[0, 0].errorbar(bal100['mix'],bal100['recallnc'], yerr = bal100['stdnc'], fmt ='o', label ='min class size = 100')\n",
    "axs[0, 0].errorbar(bal200['mix'],bal200['recallnc'], yerr = bal200['stdnc'], fmt ='o', label ='min class size = 200')\n",
    "axs[0, 0].errorbar(bal300['mix'],bal300['recallnc'], yerr = bal300['stdnc'], fmt ='o', label ='min class size = 300')\n",
    "axs[0, 0].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[0, 0].set_ylabel(\"recall for no crop\")\n",
    "axs[0, 0].legend(loc='lower left')\n",
    "\n",
    "axs[0, 1].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[0, 1].errorbar(bal0['mix'],bal0['recallsc'], yerr = bal0['stdsc'], fmt ='o', label ='min class size = 0')\n",
    "axs[0, 1].errorbar(bal100['mix'],bal100['recallsc'], yerr = bal100['stdsc'], fmt ='o', label ='min class size = 100')\n",
    "axs[0, 1].errorbar(bal200['mix'],bal200['recallsc'], yerr = bal200['stdsc'], fmt ='o', label ='min class size = 200')\n",
    "axs[0, 1].errorbar(bal300['mix'],bal300['recallsc'], yerr = bal300['stdsc'], fmt ='o', label ='min class size = 300')\n",
    "axs[0, 1].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[0, 1].set_ylabel(\"recall for small crop\")\n",
    "axs[0, 1].legend(loc='lower right')\n",
    "\n",
    "axs[1, 0].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[1, 0].errorbar(bal0['mix'],bal0['recallbc'], yerr = bal0['stdbc'], fmt ='o', label ='min class size = 0')\n",
    "axs[1, 0].errorbar(bal100['mix'],bal100['recallbc'], yerr = bal100['stdbc'], fmt ='o', label ='min class size = 100')\n",
    "axs[1, 0].errorbar(bal200['mix'],bal200['recallbc'], yerr = bal200['stdbc'], fmt ='o', label ='min class size = 200')\n",
    "axs[1, 0].errorbar(bal300['mix'],bal300['recallbc'], yerr = bal300['stdbc'], fmt ='o', label ='min class size = 300')\n",
    "axs[1, 0].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[1, 0].set_ylabel(\"recall for big crop\")\n",
    "axs[1, 0].legend(loc='lower right')\n",
    "\n",
    "axs[1, 1].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[1, 1].errorbar(bal0['mix'],bal0['avgF1'], yerr = bal0['stdF1'], fmt ='o', label ='min class size = 0')\n",
    "axs[1, 1].errorbar(bal100['mix'],bal100['avgF1'], yerr = bal100['stdF1'], fmt ='o', label ='min class size = 100')\n",
    "axs[1, 1].errorbar(bal200['mix'],bal200['avgF1'], yerr = bal200['stdF1'], fmt ='o', label ='min class size = 200')\n",
    "axs[1, 1].errorbar(bal300['mix'],bal300['avgF1'], yerr = bal300['stdF1'], fmt ='o', label ='min class size = 300')\n",
    "axs[1, 1].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[1, 1].set_ylabel(\"F1 score for crop no crop\")\n",
    "axs[1, 1].legend(loc='lower right')\n",
    "\n",
    "fig.suptitle('sample model comparisons with 25 land cover classes',y=.94)\n",
    "fig.tight_layout(pad=3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af22cc0-fc9e-4a1f-9f22-da251255086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "class MyScalarFormatter(ScalarFormatter):\n",
    "    # Override '_set_format' with your own\n",
    "    def _set_format(self):\n",
    "        self.format = '%.2f'  # Show 2 decimals\n",
    "        \n",
    "agg_score_tab = '/home/downspout-cel/paraguay_lc/classification/RF/model_stats/CEL_model_iteration_LC25_summary.csv'\n",
    "agg_scores = pd.read_csv(agg_score_tab)\n",
    "#print(agg_scores)\n",
    "agg_scores[['bal', 'mix']] = agg_scores['S'].str.split('mix', n=1, expand=True)\n",
    "agg_scores['mix']=agg_scores['mix'].astype('int')\n",
    "balance = 'bal200'\n",
    "mod_noPoly = agg_scores[(agg_scores['bal'] == balance) & (agg_scores['F'] == 'base4NoPoly')]\n",
    "mod_poly6 = agg_scores[(agg_scores['bal'] == balance) & (agg_scores['F'] == 'base4Poly6')]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "custom_formatter = MyScalarFormatter(useMathText=True)\n",
    "\n",
    "axs[0, 0].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[0, 0].errorbar(mod_noPoly['mix'],mod_noPoly['recallnc'], yerr = mod_noPoly['stdnc'], fmt ='o', label ='mod_noPoly')\n",
    "axs[0, 0].errorbar(mod_poly6['mix'],mod_poly6['recallnc'], yerr = mod_poly6['stdnc'], fmt ='o', label ='mod_poly6')\n",
    "axs[0, 0].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[0, 0].set_ylabel(\"recall for no crop\")\n",
    "axs[0, 0].legend(loc='lower left')\n",
    "\n",
    "axs[0, 1].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[0, 1].errorbar(mod_noPoly['mix'],mod_noPoly['recallsc'], yerr = mod_noPoly['stdsc'], fmt ='o', label ='mod_noPoly')\n",
    "axs[0, 1].errorbar(mod_poly6['mix'],mod_poly6['recallsc'], yerr = mod_poly6['stdsc'], fmt ='o', label ='mod_poly6')\n",
    "axs[0, 1].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[0, 1].set_ylabel(\"recall for small crop\")\n",
    "axs[0, 1].legend(loc='lower right')\n",
    "\n",
    "axs[1, 0].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[1, 0].errorbar(mod_noPoly['mix'],mod_noPoly['recallbc'], yerr = mod_noPoly['stdbc'], fmt ='o', label ='mod_noPoly')\n",
    "axs[1, 0].errorbar(mod_poly6['mix'],mod_poly6['recallbc'], yerr = mod_poly6['stdbc'], fmt ='o', label ='mod_poly6')\n",
    "axs[1, 0].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[1, 0].set_ylabel(\"recall for big crop\")\n",
    "axs[1, 0].legend(loc='lower right')\n",
    "\n",
    "axs[1, 1].yaxis.set_major_formatter(custom_formatter)\n",
    "axs[1, 1].errorbar(mod_noPoly['mix'],mod_noPoly['avgF1'], yerr = mod_noPoly['stdF1'], fmt ='o', label ='mod_noPoly')\n",
    "axs[1, 1].errorbar(mod_poly6['mix'],mod_poly6['avgF1'], yerr = mod_poly6['stdF1'], fmt ='o', label ='mod_poly6')\n",
    "axs[1, 1].set_xlabel(\"% mixed crop included in training sample\")\n",
    "axs[1, 1].set_ylabel(\"F1 socre for crop no crop\")\n",
    "axs[1, 1].legend(loc='lower right')\n",
    "\n",
    "fig.suptitle('sample model comparisons with 25 land cover classes',y=.94)\n",
    "fig.tight_layout(pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71200ac6-aed3-47ca-8aa0-6a33fd8ef2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_models(keep_models, temp_mod_dir, main_mod_dir):\n",
    "## moves keep models into main model dir\n",
    "    \n",
    "    import shutil\n",
    "    keepers = list(keep_models.index.values)\n",
    "    print(keepers)\n",
    "\n",
    "    for k in keepers:\n",
    "        print(f'copying model for {k} to permamant directory')\n",
    "        if len(k.split('_'))==2:\n",
    "            run = ''\n",
    "        else:\n",
    "            run = k.split('_')[2]\n",
    "        model = str(k.split('_')[0]+'_'+k.split('_')[1])\n",
    "        current_file = os.path.join(temp_mod_dir,f'{model}_LC25_RFmod{run}.joblib')\n",
    "        final_file = os.path.join(main_mod_dir,f'{model}_21_LC25_RFmod.joblib')\n",
    "        shutil.copy(current_file, final_file)\n",
    "\n",
    "#save_best_models(keep_models, mod_dir, classification_params['main_model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502834d3-8824-47f3-83c0-299d39e7a724",
   "metadata": {},
   "source": [
    "### For multi-year model: Merge dfs for multiple years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e86156-93d5-44ae-9f55-49927f3fe571",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'{feature_model}_{sample_model}'\n",
    "\n",
    "years = [2017,2024]\n",
    "df_list = []\n",
    "for y in range(years[0],years[1]):\n",
    "    print(y)\n",
    "    pixdf_path = f'/home/downspout-cel/paraguay_lc/classification/inputs/pixdf_{model_name}_{year}.csv'\n",
    "    pixdf = pd.read_csv(pixdf_path)\n",
    "    vardf = pixdf.filter(regex='var_')\n",
    "    nancols = vardf.columns[vardf.isna().any()].tolist()\n",
    "    if len(nanocls) > 0:\n",
    "        print('oops -- NaNs in:', nancols)\n",
    "    df_list.append(pixdf)\n",
    "allpix = pd.concat(df_list)\n",
    "allpix_path = f'/home/downspout-cel/paraguay_lc/classification/inputs/pixdf_{model_name}_9999.csv'\n",
    "#pd.DataFrame.to_csv(allpix, allpix_path)\n",
    "print(all_pts.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6803d4-aa7a-42bd-b695-9bafcc4d97f4",
   "metadata": {},
   "source": [
    "## Basic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696938fd",
   "metadata": {},
   "source": [
    "#### View the look up table\n",
    "These are the different LC_models to group things in classification and to translate between numerical map categories and text labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lut=pd.read_csv('../Class_LUT.csv')\n",
    "lut.drop(['Description'], axis=1, inplace=True)\n",
    "\n",
    "print(lut.sort_values('LC_UNQ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d87ac",
   "metadata": {},
   "source": [
    "## view confusion matrices\n",
    "Note parameters: (pred_col, obs_col, lut, lc_mod_map, lc_mod_acc, print_cm=False, out_dir=None, model_name=None)\n",
    "To print cm to csv file, change print_cm to True and provide an out_dir and model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: if running build_weighted_accuracy_table below, these will be printed to file within that.\n",
    "'''\n",
    "\n",
    "cm_cropNoCrop = get_confusion_matrix(rf0[1]['pred'],rf0[1]['label'],lut,classification_params['lc_mod'],'cropNoCrop', \n",
    "                                     print_cm=False, out_dir=classification_params['model_dir'],\n",
    "                                     model_name=classification_params['model_name'])\n",
    "cm_cropType = get_confusion_matrix(rf0[1]['pred'],rf0[1]['label'],lut,classification_params['lc_mod'],'cropType', \n",
    "                                   print_cm=False, out_dir=classification_params['model_dir'],\n",
    "                                   model_name=classification_params['model_name'])\n",
    "cm_veg = get_confusion_matrix(rf0[1]['pred'],rf0[1]['label'],lut,classification_params['lc_mod'],'veg', \n",
    "                              print_cm=False, out_dir=classification_params['model_dir'],\n",
    "                              model_name=classification_params['model_name'])\n",
    "cm_all = get_confusion_matrix(rf0[1]['pred'],rf0[1]['label'],lut,classification_params['lc_mod'],'all', \n",
    "                              print_cm=False, out_dir=classification_params['model_dir'],\n",
    "                              model_name=classification_params['model_name'])\n",
    "cm_single = get_confusion_matrix(rf0[1]['pred'],rf0[1]['label'],lut,classification_params['lc_mod'],classification_params['lc_mod'],False,classification_params['model_dir'],None)\n",
    "\n",
    "print(cm_cropNoCrop)\n",
    "print(cm_cropType)\n",
    "print(cm_veg)\n",
    "print(cm_all)\n",
    "print(cm_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071eca7",
   "metadata": {},
   "source": [
    "## view variable importance\n",
    "this can be computed via Impurity or Permutation method (see sklearn docs)  by setting impmeth in rf_model\n",
    "The full list is stored in the model directory for further manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_path = os.path.join(classification_params['model_dir'],'VarImportance_{}.csv'.format(classification_params['model_name']))\n",
    "var_imp = pd.read_csv(var_imp_path, names=['var','imp'], header=None)\n",
    "## view 10 most important variables:\n",
    "var_imp.sort_values('imp', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classification_params['feature_mod_dict'], 'r+') as feature_model_dict:\n",
    "    dic = json.load(feature_model_dict)\n",
    "print(dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfafd57",
   "metadata": {},
   "source": [
    "## Build weighted accuracy matrix for model selection / optimization\n",
    "#### Note, this is incorporated within automated methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b824df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mc_holdout = os.path.join(classification_params['main_model_dir'],'{}_mixedCrop_HO20.csv'.format(classification_params['feature_model']))\n",
    "#model_name = classification_params['model_name']\n",
    "#model_name = model_name \n",
    "#out_dir = os.path.join(classification_params['local_dir'],'cmsbi')\n",
    "                          \n",
    "#wacc = build_weighted_accuracy_table(out_dir,model_name,rf0,classification_params[\"pixdf\"],lut)\n",
    "#wacc = build_weighted_accuracy_table(out_dir,model_name,rf0,classification_params[\"pixdf\"],lut,binary=True, second_cm=False, ho_path=None)\n",
    "#print(wacc.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179ef0f-beba-432e-bb39-73d0fa4c701c",
   "metadata": {},
   "source": [
    "#### create noPoly datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b6619-47fc-4cb0-8938-e7dbe76a4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vardir = '/home/downspout-cel/paraguay_lc/vector/pts_training/features'\n",
    "#vardir = '/home/downspout-cel/paraguay_lc/classification/inputs'\n",
    "vardir = '/home/downspout-cel/paraguay_lc/vector/pts_calval/fixed_HOs'\n",
    "#dfs = [f for f in os.listdir(vardir) if f.startswith(\"ptsfeats_base4Poly6\")]\n",
    "#dfs = [f for f in os.listdir(vardir) if f.startswith(\"pixdf_base4Poly6\")]\n",
    "dfs = [f for f in os.listdir(vardir) if f.startswith(\"base4Poly6\")]\n",
    "for df in dfs:\n",
    "    df_in = os.path.join(vardir,df)\n",
    "    newname = df.replace(\"Poly6\",\"NoPoly\")\n",
    "    df_out = os.path.join(vardir,newname)\n",
    "    df1 = pd.read_csv(df_in)\n",
    "    #print(df1.columns.values.tolist())\n",
    "    poly_vars = [v for v in df1.columns.values.tolist() if v.startswith('var_poly')]\n",
    "    #df1.drop(['var_poly_ext', 'var_poly_dst', 'var_poly_cropbnds', 'var_poly_area', 'var_poly_APrEf', 'var_poly_NovDecStd','var_poly_APR'], axis=1, inplace=True)\n",
    "    df1.drop(poly_vars,axis=1,inplace=True)\n",
    "    df1.to_csv(df_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9d11e",
   "metadata": {},
   "source": [
    "## To save an html copy of this notebook with all outputs:\n",
    "(these two cells should be last in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35471d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "### comment out above line and run this cell to print output as html\n",
    "\n",
    "out_name = str(basic_config['country']+'6c_RandomFoest_ModelComparisons'+'_model'+str(classification_params['model_name'])+'basic_config['filter_yr'])\n",
    "!jupyter nbconvert --output-dir='./Outputs' --to html --no-input --ExecutePreprocessor.store_widget_state=True --output=$out_name 6c_RandomFoest_ModelComparisons.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075d3f8-981c-42e2-aa5b-9fd20e2cf664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
