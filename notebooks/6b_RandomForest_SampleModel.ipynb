{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a947286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "#import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d48bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"../LUCinSA_helpers\")\n",
    "from rf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f479b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMETERS: modify in notebook_params notebook, then run that notebook and this cell to update here\n",
    "DO NOT modify this cell\n",
    "'''\n",
    "\n",
    "%store -r basic_config\n",
    "#print(\" modelling year is (filter_year param): {} (this is first year if season spans two years)\".format(basic_config['filter_yr']))\n",
    "%store -r classification_params\n",
    "\n",
    "print(\"Classification_Params: \\n\" \n",
    "      \" temp output files are saved to (local_model_dir): {} \\n\" \n",
    "      \" shared modelling files are in (main_model_dir): {} \\n\" \n",
    "      \" feature_model = {} \\n sample_model = {} \\n model_name = {} \\n\"\n",
    "      \" the full sample pt file: {} \\n\"\n",
    "      \" the full sample dataframe with the feature model applied: {} \\n\"\n",
    "      \" the subset pt file based on the sample model: {} \\n\"\n",
    "      \" sample_model_dict: {} \\n lc_class = {}\"\n",
    "      .format(classification_params['model_dir'],classification_params['main_model_dir'],classification_params['feature_model'],\n",
    "              classification_params['sample_model'],classification_params['model_name'],basic_config['ptfile'],\n",
    "              classification_params['samp_pix_vars'],classification_params['samp_pts'],\n",
    "              classification_params['feature_mod_dict'],classification_params['lc_mod']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878c7cf",
   "metadata": {},
   "source": [
    "## define / alter sample pixels to participate in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb25ba",
   "metadata": {},
   "source": [
    "### load in LUT to see class options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "lut=Path('../Class_LUT.csv')\n",
    "print(pd.read_csv(lut).sort_values('LC_UNQ')[['LC_UNQ','USE_NAME','LC25','LC25_name','LC3_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0d401",
   "metadata": {},
   "source": [
    "### start with default models with pixel-only data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_pts = pd.read_csv(basic_config['ptfile'])\n",
    "pix_vars = pd.read_csv(classification_params['samp_pix_vars'])\n",
    "#print(samp_pts)\n",
    "\n",
    "pix_data = pix_vars.merge(samp_pts, left_on='OID_', right_on='OID_', how='left')\n",
    "\n",
    "#pix_data.drop(['LC2'], axis=1, inplace=True)\n",
    "pixdf = pix_data.merge(pd.read_csv(lut), left_on='Class', right_on='USE_NAME', how='left')\n",
    "print('sample breakdown by LC25 class:')\n",
    "print(pixdf['LC25_name'].value_counts())\n",
    "\n",
    "print('default rf model with all sample pixels and pixel only data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfb956",
   "metadata": {},
   "source": [
    "#### check for nan columns if desired\n",
    "Note that NaN is a problem in columns that start with 'Var' (as these are used in the rf model). \n",
    "Columns that do not start with 'Var' are probably ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deface16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note if any NaN columns start with 'var_', the NaNs will cause the rf model to fail\n",
    "nancols = pixdf.columns[pixdf.isna().any()].tolist()\n",
    "print(f'columns with NaN: {nancols}')\n",
    "var_nans = [v for v in nancols if v.startswith('var_')]\n",
    "if len(var_nans) > 0:\n",
    "    print (f'WARNING: The following variables have NaN and will not work in RF model: {var_nans}')\n",
    "else:\n",
    "    print('none of these are model variables, so can probably be ignored')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fbc9e",
   "metadata": {},
   "source": [
    "## Add smallholder flag to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# already done at level of point file (might need to do again if creating new pt file)\n",
    "def apply_smalls(pixdf,lut,outpath=None):\n",
    "    if 'LC2' not in list(pixdf.columns):\n",
    "        pixdf = pixdf.merge(lut[['LC_UNQ','LC2']],on='LC_UNQ',how='left')\n",
    "    ### <=1 hectare\n",
    "    pixdf['smlhld_1ha'] = pixdf.apply(lambda x: 1 if (\n",
    "        ((x['var_poly_area'] < 100) and (x['LC2'] == 1)) or (\n",
    "        (x['Width'] <= 100) and (x['LC2'] == 1)) or x['LC25'] in ['Crops-mix','Crops-Mandioca','Crops-Horticulture','Crops-Sesame']) else 0, axis=1)\n",
    "    num_smlhld_1ha = pixdf['smlhld_1ha'].sum()\n",
    "    print(f'{num_smlhld_1ha} of the sample points are small fields < 100 m across')\n",
    "    ### <= .5 hectare\n",
    "    pixdf['smlhld_halfha'] = pixdf.apply(lambda x: 1 if (\n",
    "        ((x['var_poly_area'] < 50) and (x['LC2'] == 1)) or (\n",
    "        (x['Width'] <= 50) and (x['LC2'] == 1)) or x['LC25'] in ['Crops-mix','Crops-Mandioca','Crops-Horticulture','Crops-Sesame']) else 0, axis=1)\n",
    "    num_smlhld_halfha = pixdf['smlhld_halfha'].sum()\n",
    "    print(f'{num_smlhld_halfha} of the sample points are very small fields < 50 m across')\n",
    "    if outpath:\n",
    "        pd.DataFrame.to_csv(pixdf, outpath)\n",
    "    \n",
    "    return pixdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e6307",
   "metadata": {},
   "source": [
    "## Separate set of mixed crop pts to use as fixed holdout "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6399140",
   "metadata": {},
   "source": [
    "Note: This was already done. A fixed holdout was separated from the full pt file to avoid confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_ho_dir = '/home/downspout-cel/paraguay_lc/vector/pts_calval/fixed_HOs'\n",
    "ho, tr0 = get_stable_holdout(pixdf, fixed_ho_dir, 20, 'smallCrop', lut, overwrite=False) \n",
    "#ho1, tr1 = get_stable_holdout(tr0, fixed_ho_dir, 20, 'bigCrop', lut, overwrite=False) \n",
    "#ho2, tr2 = get_stable_holdout(tr1, fixed_ho_dir, 20, 'noCrop', lut, overwrite=False)\n",
    "#pixdf = pd.read_csv('/home/downspout-cel/paraguay_lc/vector/pts_training/GENERAL_TRAINING.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c278817",
   "metadata": {},
   "source": [
    "## prep pixel datasets by reducing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563b2d7",
   "metadata": {},
   "source": [
    "### Option1: by sampling method (reducing dominant CAN soy pts that are not verified in GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Note that there are many fewer ground verified soy points -- we want to keep all of these in the sample\n",
    "soyground = pixdf[(pixdf['LC25_name'] == 'Crops-Soybeans') & (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "print(soyground.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4122ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model_name = pixdf_25Soy\n",
    "##   removes 3/4 of the soy points because they are far overrepresented\n",
    "pixdf_25Soy = pixdf[(pixdf['rand']>.8) | (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "print('25Soy sample breakdown by LC25 class:')\n",
    "print(pixdf_25Soy['LC25_name'].value_counts())\n",
    "pixdf_path = os.path.join(classification_params['model_dir'],'pixdf_25Soy.csv')\n",
    "pd.DataFrame.to_csv(pixdf_25Soy, pixdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520be495",
   "metadata": {},
   "source": [
    "### Drop classes that have sample sizes too small to model\n",
    "#### optional -- only uesful if modeling all classes (LC25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropClass = ['Crops-Vineyard','NewPlant']\n",
    "pixdf = pixdf[~pixdf['LC25_name'].isin(dropClass)]\n",
    "#pixdf = pixdf.drop(\"Description\", axis=1)\n",
    "pd.options.display.max_columns = None\n",
    "print(pixdf['LC25_name'].value_counts())\n",
    "#print(pd.DataFrame(pixdf.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bf5bb",
   "metadata": {},
   "source": [
    "### Option2: by balancing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405dd384",
   "metadata": {},
   "source": [
    "##### First run this to make sure ground sample points are used first for soy (because sample is overwhelmed by unverified CAN pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model name = pixdf_\n",
    "## Note we need about 1374 soy points for a balanced model. we want to include all the 356 ground points found above + \n",
    "allsoy = pixdf['LC25_name'].value_counts()['Crops-Soybeans']\n",
    "soyground = pixdf[(pixdf['LC25_name'] == 'Crops-Soybeans') & (pixdf['SampMethod'] != 'CAN - unverified in GE')].shape[0]\n",
    "othersoy = (1600 - soyground) / allsoy\n",
    "pixdf_balsoy = pixdf[(pixdf['rand'] < othersoy) | (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "print('balsoy sample breakdown by LC25 class:')\n",
    "print(pixdf_balsoy['LC25_name'].value_counts())\n",
    "#pixdf_path = os.path.join(classification_params['model_dir'],'pixdf_balsoy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5d332",
   "metadata": {},
   "source": [
    "### Option2: by balancing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4164680-a586-4b4d-8fc6-67ffbf592f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # run balance_training_data function\n",
    " #   balances class samples based on map proportion, relative to sample size for class with max map proportion\n",
    " #   (this estimated map proportion is a column named \"perLC25E\" in the LUT )\n",
    " #   allows a minimum threshold to be set {cutoff} so that sample sizes are not reduced below the minimum\n",
    " #   allows a factor to be set for mixed (heterogeneous) classes to sample them more heavily than main classes\n",
    " #       (the maximum value will depend on the available samples for these classes. Current max is ~4)\n",
    " #   prints 'pixdf_bal{cutoff}mix{mix_factor}.csv' in out_dir\n",
    "    \n",
    "cutoff = 300\n",
    "mix_factor = 5\n",
    "out_dir = '/home/downspout-cel/paraguay_lc/vector/pts_training/pt_subsets' \n",
    "pixdf_bal = balance_training_data(lut, pixdf_balsoy, out_dir, cutoff = cutoff, mix_factor = mix_factor)\n",
    "## repeat with mix_factor = 2 - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3d9fa",
   "metadata": {},
   "source": [
    "## Strip excess columns from pixdf\n",
    "to avoid name changes when joining with tables that already have these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0702c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pixdf_bal\n",
    "#df = pd.read_csv(os.path.join(classification_params['main_model_dir'],'pixdf_bal100mix0.csv'))\n",
    "out_name = 'bal{}mix{}.csv'.format(cutoff,mix_factor)\n",
    "df = df.loc[:,~df.columns.str.contains('var')] \n",
    "df = df.loc[:,~df.columns.str.endswith('_y')] \n",
    "df = df.rename(columns=lambda x: x.replace('_x', ''))\n",
    "df.drop(['Description', 'ratios','Segmentation','LCTrans','LCTrans_name'], axis=1, inplace=True)\n",
    "print(df.columns.tolist())\n",
    "df.to_csv(os.path.join(out_dir,out_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027898d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is an old method used for the original bal1000 model. Better to use the balance_training_data function\n",
    "\n",
    "#### sample model_name = bal1000\n",
    "pixdf1 = pixdf[(pixdf['PurePixel'] != 'No') | (pixdf['LC25_name'].str.contains('mix', na=False, case=False))]\n",
    "pixdf2 = pixdf1[(pixdf['rand']>.84) | (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "pixdf3 = pixdf2[(pixdf2['rand']>.65) | (pixdf2['LC25_name'] != 'Mixed-VegEdge')]\n",
    "pixdf4 = pixdf3[(pixdf3['rand']>.65) | (pixdf3['LC25_name'] != 'Crops-mix')]\n",
    "pixdf5 = pixdf4[(pixdf4['rand']>.86) | (pixdf4['LC25_name'] != 'Mixed-path')]\n",
    "pixdf6 = pixdf5[(pixdf5['rand']>.30) | (pixdf5['LC25_name'] != 'Crops-Yerba-Mate')]\n",
    "pixdf7 = pixdf6[(pixdf6['rand']>.39) | (pixdf6['SampMethod'] == 'GE_KW_sup') | (pixdf6['LC25_name'] != 'Grassland-Managed')]\n",
    "#pixdf8 = pixdf7[(pixdf7['rand']>.36) | (pixdf7['LC25_name'] != 'Trees-Forest')]\n",
    "print('pixdf_bal0 sample breakdown by LC25 class:')\n",
    "print(pixdf7['LC25_name'].value_counts())\n",
    "pixdf_path = os.path.join(classification_params['main_model_dir'],'pixdf_base1000.csv')\n",
    "pd.DataFrame.to_csv(pixdf7, pixdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247eb48",
   "metadata": {},
   "source": [
    "## if polygons are available, can combine pixel and polygon dfs and create rf datasets for points with polygons and those without:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_data = pd.read_csv(classification_params['samp_poly'])\n",
    "#rename column names that also occur in pixel df\n",
    "poly_data.rename(columns={'area':'areaSeg'}, inplace=True)\n",
    "all_data = pixdf.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "polypixdf_path = os.path.join(classification_params['model_dir'],'pts_polyData_joinCheck.csv')\n",
    "pd.DataFrame.to_csv(all_data, polypixdf_path, sep=',', na_rep='NaN', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad21203",
   "metadata": {},
   "source": [
    "### first create dataset for points outside of polygons (here we have no variables to add to the original model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideSeg = all_data[all_data['areaSeg'].isna()]\n",
    "print(f'of the {all_data.shape[0]} sample points in our dataset, {outsideSeg.shape[0]} are outside of our segmented polygons')\n",
    "print(outsideSeg['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'outside')\n",
    "rfout17 = rf_model(outsideSeg,out_dir,'All','Permutation',29,'Fullsamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ef9f7",
   "metadata": {},
   "source": [
    "And for model with more balanced soy representation (25Soy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_25Soy = pixdf_25Soy.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "outsideSeg_25Soy = all_data_25Soy[all_data_25Soy['areaSeg'].isna()]\n",
    "print(outsideSeg_25Soy['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'outside')\n",
    "rfout17_lessSoy = rf_model(outsideSeg_lessSoy,out_dir,'All','Permutation',29,'LessSoy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af312b5",
   "metadata": {},
   "source": [
    "#### now create dataset for points inside of polygons (here we want to add some variables first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135283bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly_data['AvgU'] = poly_data.apply(lambda x:count([x[c] for c in df.columns if c.endswith('U')]),axis=1)\n",
    "#TODO: calculate these in pandas as above\n",
    "poly_data.rename(columns={'areaSeg':'var_areaSeg','AVGU':'var_AVGU','AVGR':'var_AVGR','AVGSTD':'var_AVGSTD','MAXR':'var_MAXR','MINR':'var_MINR','STDU':'var_STDU','MINU':'var_MINU','MAXU':'var_MAXU','rU':'var_RU'}, inplace=True)\n",
    "polyvars = pixdf.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "withinSeg = polyvars[polyvars['var_areaSeg'] > 0]\n",
    "\n",
    "print(withinSeg['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'within')\n",
    "#rfin17 = rf_model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cd9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_lessSoy = pixdf_lessSoy.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "withinSeg_lessSoy = all_data_lessSoy[all_data_lessSoy['var_areaSeg']>0]\n",
    "print(withinSeg_lessSoy['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'within')\n",
    "#rfin17_lessSoy = rf_model(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5fe33",
   "metadata": {},
   "source": [
    "## To save an html copy of this notebook with all outputs:\n",
    "(these two cells should be last in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "### comment out above line and run this cell to print output as html\n",
    "\n",
    "out_name = str(basic_config['country']+'6a_RandomFoest_VariableDataframe'+'_model'+str(classification_params['model_name'])+'_'+'Tests1')\n",
    "!jupyter nbconvert --output-dir='./Outputs' --to html --no-input --ExecutePreprocessor.store_widget_state=True --output=$out_name 6b_RandomFoest_ModelComparisons.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
