{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a947286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "#import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d48bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"../LUCinSA_helpers\")\n",
    "from rf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f479b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMETERS: modify in notebook_params notebook, then run that notebook and this cell to update here\n",
    "DO NOT modify this cell\n",
    "'''\n",
    "\n",
    "%store -r basic_config\n",
    "#print(\" modelling year is (filter_year param): {} (this is first year if season spans two years)\".format(basic_config['filter_yr']))\n",
    "%store -r classification_params\n",
    "\n",
    "print(\"Classification_Params: \\n\" \n",
    "      \" temp output files are saved to (local_model_dir): {} \\n\" \n",
    "      \" shared modelling files are in (main_model_dir): {} \\n\" \n",
    "      \" feature_model = {} \\n sample_model = {} \\n model_name = {} \\n\"\n",
    "      \" the full sample pt file: {} \\n\"\n",
    "      \" the full sample dataframe with the feature model applied: {} \\n\"\n",
    "      \" the subset pt file based on the sample model: {} \\n\"\n",
    "      \" sample_model_dict: {} \\n lc_class = {}\"\n",
    "      .format(classification_params['model_dir'],classification_params['main_model_dir'],classification_params['feature_model'],\n",
    "              classification_params['sample_model'],classification_params['model_name'],basic_config['ptfile'],\n",
    "              classification_params['samp_pix_vars'],classification_params['samp_pts'],\n",
    "              classification_params['feature_mod_dict'],classification_params['lc_mod']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878c7cf",
   "metadata": {},
   "source": [
    "## define / alter sample pixels to participate in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb25ba",
   "metadata": {},
   "source": [
    "### load in LUT to see class options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "lut=Path('../Class_LUT.csv')\n",
    "print(pd.read_csv(lut).sort_values('LC_UNQ')[['LC_UNQ','USE_NAME','LC25','LC25_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0d401",
   "metadata": {},
   "source": [
    "### start with default models with pixel-only data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_vars = pd.read_csv(classification_params['samp_pix_vars'])\n",
    "print('pix_vars has {} features and {} sample points'.format(pix_vars.shape[1]-1, pix_vars.shape[0]))\n",
    "print(pix_vars.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_pts = pd.read_csv(basic_config['ptfile'])\n",
    "pix_vars = pd.read_csv(classification_params['samp_pix_vars'])\n",
    "#print(samp_pts)\n",
    "\n",
    "pix_data = pix_vars.merge(samp_pts, left_on='OID_', right_on='OID_', how='left')\n",
    "\n",
    "pix_data.drop(['LC2'], axis=1, inplace=True)\n",
    "pixdf = pix_data.merge(pd.read_csv(lut), left_on='Class', right_on='USE_NAME', how='left')\n",
    "print('sample breakdown by LC25 class:')\n",
    "print(pixdf['LC25_name'].value_counts())\n",
    "\n",
    "print('default rf model with all sample pixels and pixel only data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fbc9e",
   "metadata": {},
   "source": [
    "## Add smallholder flag to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <=1 hectare\n",
    "pixdf['smlhld_1ha'] = pixdf.apply(lambda x: 1 if (\n",
    "    ((x['var_poly_pred_area'] < 100) and (x['LC2'] == 1)) or (\n",
    "    (x['FieldWidth'] <= 100) and (x['LC2'] == 1)) or x['LC25'] == 'Crops-mix') else 0, axis=1)\n",
    "print(pixdf['smlhld_1ha'].value_counts())\n",
    "## <= .5 hectare\n",
    "pixdf['smlhld_halfha'] = pixdf.apply(lambda x: 1 if (\n",
    "    ((x['var_poly_pred_area'] < 50) and (x['LC2'] == 1)) or (\n",
    "    (x['FieldWidth'] <= 50) and (x['LC2'] == 1)) or x['LC25'] == 'Crops-mix') else 0, axis=1)\n",
    "print(pixdf['smlhld_halfha'].value_counts())\n",
    "#pd.DataFrame.to_csv(pixdf, os.path.join([classification_params['model_dir']/ptsgb_withSmalls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913efc8",
   "metadata": {},
   "source": [
    "## Drop classes that have sample sizes too small to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d111a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropClass = ['Crops-Vineyard','NewPlant']\n",
    "pixdf = pixdf[~pixdf['LC25_name'].isin(dropClass)]\n",
    "#pixdf = pixdf.drop(\"Description\", axis=1)\n",
    "pd.options.display.max_columns = None\n",
    "print(pixdf['LC25_name'].value_counts())\n",
    "#print(pd.DataFrame(pixdf.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597eb6d",
   "metadata": {},
   "source": [
    "#### check for nan columns if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note if any NaN columns start with 'var_', the NaNs will cause the rf model to fail\n",
    "nancols = pixdf.columns[pixdf.isna().any()].tolist()\n",
    "print(f'columns with NaN: {nancols}')\n",
    "\n",
    "#for i in nancols:\n",
    "#    print(pixdf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c278817",
   "metadata": {},
   "source": [
    "## prep pixel datasets by reducing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563b2d7",
   "metadata": {},
   "source": [
    "### Option1: by sampling method (reducing dominant CAN soy pts that are not verified in GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Note that there are many fewer ground verified soy points -- we want to keep all of these in the sample\n",
    "soyground = pixdf[(pixdf['LC25_name'] == 'Crops-Soybeans') & (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "print(soyground.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4122ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model_name = pixdf_25Soy\n",
    "##   removes 3/4 of the soy points because they are far overrepresented\n",
    "pixdf_25Soy = pixdf[(pixdf['rand']>.8) | (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "print('25Soy sample breakdown by LC25 class:')\n",
    "print(pixdf_25Soy['LC25_name'].value_counts())\n",
    "pixdf_path = os.path.join(classification_params['model_dir'],'pixdf_25Soy.csv')\n",
    "pd.DataFrame.to_csv(pixdf_25Soy, pixdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bf5bb",
   "metadata": {},
   "source": [
    "### Option2: by balancing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405dd384",
   "metadata": {},
   "source": [
    "##### First run this to make sure ground sample points are used first for soy (because sample is overwhelmed by unverified CAN pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model name = pixdf_\n",
    "## Note we need about 1374 soy points for a balanced model. we want to include all the 356 ground points found above + \n",
    "allsoy = pixdf['LC25_name'].value_counts()['Crops-Soybeans']\n",
    "othersoy = (1550 - 356) / allsoy\n",
    "print(othersoy)\n",
    "pixdf_balsoy = pixdf[(pixdf['rand'] < othersoy) | (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "print('balsoy sample breakdown by LC25 class:')\n",
    "print(pixdf_balsoy['LC25_name'].value_counts())\n",
    "#pixdf_path = os.path.join(classification_params['model_dir'],'pixdf_balsoy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5d332",
   "metadata": {},
   "source": [
    "### Option2: by balancing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4164680-a586-4b4d-8fc6-67ffbf592f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # run balance_training_data function\n",
    " #   balances class samples based on map proportion, relative to sample size for class with max map proportion\n",
    " #   (this estimated map proportion is a column named \"perLC25E\" in the LUT )\n",
    " #   allows a minimum threshold to be set {cutoff} so that sample sizes are not reduced below the minimum\n",
    " #   allows a factor to be set for mixed (heterogeneous) classes to sample them more heavily than main classes\n",
    " #       (the maximum value will depend on the available samples for these classes. Current max is ~4)\n",
    " #   prints 'pixdf_bal{cutoff}mix{mix_factor}.csv' in out_dir\n",
    "\n",
    "out_dir = classification_params['local_dir']\n",
    "#out_dir = classification_params['main_model_dir']\n",
    "balance_training_data(lut, pixdf_balsoy, out_dir, \"rebal\", cutoff = 500, mix_factor = 8)\n",
    "## repeat with mix_factor = 2 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##fix this\n",
    "new_feature_mod = 'Max_nomon5-6'\n",
    "classification_params['samp_pix_vars'] = '/home/downspout-cel/paraguay_lc/vector/tests/ptsgdb_{}.csv'.format(new_feature_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to fix any mess ups u make \n",
    "\"\"\"\n",
    "dir = \"/home/downspout-cel/paraguay_lc/classification/RF\"\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith(\"pixdf_{}\".format(new_feature_mod)):\n",
    "        filepath = os.path.join(dir, filename)\n",
    "        os.remove(filepath)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027898d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is an old method used for the original bal1000 model. Better to use the balance_training_data function\n",
    "\n",
    "#### sample model_name = bal1000\n",
    "pixdf1 = pixdf[(pixdf['PurePixel'] != 'No') | (pixdf['LC25_name'].str.contains('mix', na=False, case=False))]\n",
    "pixdf2 = pixdf1[(pixdf['rand']>.84) | (pixdf['SampMethod'] != 'CAN - unverified in GE')]\n",
    "pixdf3 = pixdf2[(pixdf2['rand']>.65) | (pixdf2['LC25_name'] != 'Mixed-VegEdge')]\n",
    "pixdf4 = pixdf3[(pixdf3['rand']>.65) | (pixdf3['LC25_name'] != 'Crops-mix')]\n",
    "pixdf5 = pixdf4[(pixdf4['rand']>.86) | (pixdf4['LC25_name'] != 'Mixed-path')]\n",
    "pixdf6 = pixdf5[(pixdf5['rand']>.30) | (pixdf5['LC25_name'] != 'Crops-Yerba-Mate')]\n",
    "pixdf7 = pixdf6[(pixdf6['rand']>.39) | (pixdf6['SampMethod'] == 'GE_KW_sup') | (pixdf6['LC25_name'] != 'Grassland-Managed')]\n",
    "#pixdf8 = pixdf7[(pixdf7['rand']>.36) | (pixdf7['LC25_name'] != 'Trees-Forest')]\n",
    "print('pixdf_bal0 sample breakdown by LC25 class:')\n",
    "print(pixdf7['LC25_name'].value_counts())\n",
    "pixdf_path = os.path.join(classification_params['main_model_dir'],'pixdf_base1000.csv')\n",
    "pd.DataFrame.to_csv(pixdf7, pixdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247eb48",
   "metadata": {},
   "source": [
    "## if polygons are available, can combine pixel and polygon dfs and create rf datasets for points with polygons and those without:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_data = pd.read_csv(classification_params['samp_poly'])\n",
    "#rename column names that also occur in pixel df\n",
    "poly_data.rename(columns={'area':'areaSeg'}, inplace=True)\n",
    "all_data = pixdf.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "polypixdf_path = os.path.join(classification_params['model_dir'],'pts_polyData_joinCheck.csv')\n",
    "pd.DataFrame.to_csv(all_data, polypixdf_path, sep=',', na_rep='NaN', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad21203",
   "metadata": {},
   "source": [
    "### first create dataset for points outside of polygons (here we have no variables to add to the original model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideSeg = all_data[all_data['areaSeg'].isna()]\n",
    "print(f'of the {all_data.shape[0]} sample points in our dataset, {outsideSeg.shape[0]} are outside of our segmented polygons')\n",
    "print(outsideSeg['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'outside')\n",
    "rfout17 = rf_model(outsideSeg,out_dir,'All','Permutation',29,'Fullsamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ef9f7",
   "metadata": {},
   "source": [
    "And for model with more balanced soy representation (25Soy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_25Soy = pixdf_25Soy.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "outsideSeg_25Soy = all_data_25Soy[all_data_25Soy['areaSeg'].isna()]\n",
    "print(outsideSeg_25Soy['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'outside')\n",
    "rfout17_lessSoy = rf_model(outsideSeg_lessSoy,out_dir,'All','Permutation',29,'LessSoy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af312b5",
   "metadata": {},
   "source": [
    "#### now create dataset for points inside of polygons (here we want to add some variables first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135283bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly_data['AvgU'] = poly_data.apply(lambda x:count([x[c] for c in df.columns if c.endswith('U')]),axis=1)\n",
    "#TODO: calculate these in pandas as above\n",
    "poly_data.rename(columns={'areaSeg':'var_areaSeg','AVGU':'var_AVGU','AVGR':'var_AVGR','AVGSTD':'var_AVGSTD','MAXR':'var_MAXR','MINR':'var_MINR','STDU':'var_STDU','MINU':'var_MINU','MAXU':'var_MAXU','rU':'var_RU'}, inplace=True)\n",
    "polyvars = pixdf.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "withinSeg = polyvars[polyvars['var_areaSeg'] > 0]\n",
    "\n",
    "print(withinSeg['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'within')\n",
    "#rfin17 = rf_model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cd9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_lessSoy = pixdf_lessSoy.merge(poly_data, left_on='OID_', right_on='OID_', how='left')\n",
    "withinSeg_lessSoy = all_data_lessSoy[all_data_lessSoy['var_areaSeg']>0]\n",
    "print(withinSeg_lessSoy['LC17_name'].value_counts())\n",
    "out_dir = os.path.join(classification_params['model_dir'],'within')\n",
    "#rfin17_lessSoy = rf_model(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5fe33",
   "metadata": {},
   "source": [
    "## To save an html copy of this notebook with all outputs:\n",
    "(these two cells should be last in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "### comment out above line and run this cell to print output as html\n",
    "\n",
    "out_name = str(basic_config['country']+'6a_RandomFoest_VariableDataframe'+'_model'+str(classification_params['model_name'])+'_'+'Tests1')\n",
    "!jupyter nbconvert --output-dir='./Outputs' --to html --no-input --ExecutePreprocessor.store_widget_state=True --output=$out_name 6b_RandomFoest_ModelComparisons.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
